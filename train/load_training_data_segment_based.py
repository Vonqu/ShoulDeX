#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºåˆ†æ®µçš„è®­ç»ƒæ•°æ®åŠ è½½å™¨ - ShoulDexé¡¹ç›®
åŠŸèƒ½ï¼šå¯¹æ¯ä¸ªåŠ¨ä½œæ®µåˆ†åˆ«è¿›è¡Œæ»‘çª—å¤„ç†ï¼Œé¿å…è·¨æ®µæ•°æ®æ±¡æŸ“

åˆ›å»ºæ—¶é—´ï¼š2025å¹´1æœˆ
ç‰ˆæœ¬ï¼š1.0
"""

import os
import glob
import pandas as pd
import numpy as np
import torch
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import Dataset, DataLoader, TensorDataset
from typing import List, Tuple, Dict
import pickle
from pathlib import Path


class SegmentBasedDataset(Dataset):
    """åŸºäºåˆ†æ®µçš„æ•°æ®é›†ç±»"""
    
    def __init__(self, 
                 data_folder: str,
                 window_size: int = 60,
                 time_step: int = 1,
                 sensor_cols_range: Tuple[int, int] = (1, 15),
                 angle_cols_range: Tuple[int, int] = (15, 42)):
        """
        åˆå§‹åŒ–åˆ†æ®µæ•°æ®é›†
        
        Args:
            data_folder (str): æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
            window_size (int): æ—¶é—´çª—å£å¤§å°
            time_step (int): æ—¶é—´æ­¥é•¿
            sensor_cols_range (tuple): ä¼ æ„Ÿå™¨åˆ—èŒƒå›´ (å¼€å§‹åˆ—, ç»“æŸåˆ—)
            angle_cols_range (tuple): è§’åº¦åˆ—èŒƒå›´ (å¼€å§‹åˆ—, ç»“æŸåˆ—)
        """
        self.data_folder = Path(data_folder)
        self.window_size = window_size
        self.time_step = time_step
        self.sensor_cols_range = sensor_cols_range
        self.angle_cols_range = angle_cols_range
        
        # å­˜å‚¨æ‰€æœ‰çª—å£æ•°æ®
        self.windows_x = []  # ä¼ æ„Ÿå™¨æ•°æ®çª—å£
        self.windows_y = []  # è§’åº¦æ•°æ®çª—å£
        self.segment_info = []  # è®°å½•æ¯ä¸ªçª—å£å±äºå“ªä¸ªæ–‡ä»¶
        
        # åŠ è½½å¹¶å¤„ç†æ•°æ®
        self._load_and_process_data()
    
    def _load_and_process_data(self):
        """åŠ è½½å¹¶å¤„ç†æ•°æ®"""
        print(f"ğŸ” æ­£åœ¨æ‰«ææ–‡ä»¶å¤¹: {self.data_folder}")
        
        # è·å–æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = list(self.data_folder.glob("*.csv"))
        csv_files.sort()  # ç¡®ä¿é¡ºåºä¸€è‡´
        
        if not csv_files:
            raise ValueError(f"åœ¨ {self.data_folder} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        
        print(f"ğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
        
        total_windows = 0
        
        for file_idx, csv_file in enumerate(csv_files):
            print(f"ğŸ“Š å¤„ç†æ–‡ä»¶ {file_idx+1}/{len(csv_files)}: {csv_file.name}")
            
            # è¯»å–å•ä¸ªCSVæ–‡ä»¶
            df = pd.read_csv(csv_file)
            
            # æå–ä¼ æ„Ÿå™¨å’Œè§’åº¦æ•°æ®
            sensor_data = df.iloc[:, self.sensor_cols_range[0]:self.sensor_cols_range[1]].values
            angle_data = df.iloc[:, self.angle_cols_range[0]:self.angle_cols_range[1]].values
            
            # å¯¹å½“å‰æ®µè¿›è¡Œæ»‘çª—å¤„ç†
            segment_windows_x, segment_windows_y = self._create_windows_for_segment(
                sensor_data, angle_data
            )
            
            # æ·»åŠ åˆ°æ€»æ•°æ®é›†
            self.windows_x.extend(segment_windows_x)
            self.windows_y.extend(segment_windows_y)
            
            # è®°å½•æ®µä¿¡æ¯
            segment_windows_count = len(segment_windows_x)
            self.segment_info.extend([{
                'file_name': csv_file.name,
                'file_index': file_idx,
                'segment_length': len(sensor_data),
                'window_index_in_segment': i
            } for i in range(segment_windows_count)])
            
            total_windows += segment_windows_count
            print(f"   âœ“ ç”Ÿæˆ {segment_windows_count} ä¸ªçª—å£")
        
        print(f"ğŸ¯ æ€»å…±ç”Ÿæˆ {total_windows} ä¸ªè®­ç»ƒçª—å£")
        print(f"ğŸ“ æ¯ä¸ªçª—å£å¤§å°: {self.window_size}")
        print(f"â­ï¸ æ—¶é—´æ­¥é•¿: {self.time_step}")
    
    def _create_windows_for_segment(self, sensor_data: np.ndarray, angle_data: np.ndarray) -> Tuple[List, List]:
        """
        å¯¹å•ä¸ªæ•°æ®æ®µåˆ›å»ºæ»‘åŠ¨çª—å£
        
        Args:
            sensor_data (np.ndarray): ä¼ æ„Ÿå™¨æ•°æ® [æ—¶é—´, ä¼ æ„Ÿå™¨ç»´åº¦]
            angle_data (np.ndarray): è§’åº¦æ•°æ® [æ—¶é—´, è§’åº¦ç»´åº¦]
            
        Returns:
            Tuple[List, List]: (ä¼ æ„Ÿå™¨çª—å£åˆ—è¡¨, è§’åº¦çª—å£åˆ—è¡¨)
        """
        windows_x = []
        windows_y = []
        
        # è®¡ç®—å¯ä»¥ç”Ÿæˆçš„çª—å£æ•°é‡
        max_start_idx = len(sensor_data) - self.window_size
        
        if max_start_idx < 0:
            print(f"   âš ï¸ è­¦å‘Š: æ•°æ®æ®µé•¿åº¦ {len(sensor_data)} å°äºçª—å£å¤§å° {self.window_size}ï¼Œè·³è¿‡è¯¥æ®µ")
            return windows_x, windows_y
        
        # ç”Ÿæˆæ»‘åŠ¨çª—å£
        for start_idx in range(0, max_start_idx + 1, self.time_step):
            end_idx = start_idx + self.window_size
            
            # æå–çª—å£æ•°æ®
            window_x = sensor_data[start_idx:end_idx]  # [window_size, sensor_dim]
            window_y = angle_data[end_idx - 1]  # ä½¿ç”¨çª—å£æœ€åä¸€ä¸ªæ—¶é—´ç‚¹çš„è§’åº¦ä½œä¸ºç›®æ ‡
            
            windows_x.append(window_x)
            windows_y.append(window_y)
        
        return windows_x, windows_y
    
    def __len__(self):
        return len(self.windows_x)
    
    def __getitem__(self, idx):
        return torch.FloatTensor(self.windows_x[idx]), torch.FloatTensor(self.windows_y[idx])
    
    def get_segment_info(self, idx):
        """è·å–æŒ‡å®šç´¢å¼•çš„æ®µä¿¡æ¯"""
        return self.segment_info[idx]
    
    def get_all_data(self):
        """è·å–æ‰€æœ‰æ•°æ®ï¼Œç”¨äºå½’ä¸€åŒ–"""
        all_x = np.array(self.windows_x)  # [num_windows, window_size, sensor_dim]
        all_y = np.array(self.windows_y)  # [num_windows, angle_dim]
        return all_x, all_y


class SegmentBasedDataLoader:
    """åŸºäºåˆ†æ®µçš„æ•°æ®åŠ è½½å™¨ç®¡ç†ç±»"""
    
    def __init__(self, 
                 train_folder: str,
                 test_folder: str,
                 window_size: int = 60,
                 time_step: int = 1,
                 sensor_cols_range: Tuple[int, int] = (1, 15),
                 angle_cols_range: Tuple[int, int] = (15, 42),
                 batch_size: int = 256,
                 validation_split: float = 0.2,
                 scaler_save_dir: str = "./predict/scaler",
                 trial_id: str = "segment_based"):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            train_folder (str): è®­ç»ƒæ•°æ®æ–‡ä»¶å¤¹
            test_folder (str): æµ‹è¯•æ•°æ®æ–‡ä»¶å¤¹
            window_size (int): æ—¶é—´çª—å£å¤§å°
            time_step (int): æ—¶é—´æ­¥é•¿
            sensor_cols_range (tuple): ä¼ æ„Ÿå™¨åˆ—èŒƒå›´
            angle_cols_range (tuple): è§’åº¦åˆ—èŒƒå›´
            batch_size (int): æ‰¹å¤„ç†å¤§å°
            validation_split (float): éªŒè¯é›†æ¯”ä¾‹
            scaler_save_dir (str): å½’ä¸€åŒ–å™¨ä¿å­˜ç›®å½•
            trial_id (str): è¯•éªŒIDï¼Œç”¨äºä¿å­˜æ–‡ä»¶å
        """
        self.train_folder = train_folder
        self.test_folder = test_folder
        self.window_size = window_size
        self.time_step = time_step
        self.sensor_cols_range = sensor_cols_range
        self.angle_cols_range = angle_cols_range
        self.batch_size = batch_size
        self.validation_split = validation_split
        self.scaler_save_dir = Path(scaler_save_dir)
        self.trial_id = trial_id
        
        # åˆ›å»ºå½’ä¸€åŒ–å™¨ä¿å­˜ç›®å½•
        self.scaler_save_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®é›†
        self.train_dataset = None
        self.test_dataset = None
        self.sensor_scaler = None
        self.angle_scaler = None
        
        # æ•°æ®åŠ è½½å™¨
        self.train_loader = None
        self.val_loader = None
        self.test_loader = None
    
    def load_and_prepare_data(self):
        """åŠ è½½å¹¶å‡†å¤‡æ•°æ®"""
        print("ğŸš€ å¼€å§‹åŠ è½½åŸºäºåˆ†æ®µçš„è®­ç»ƒæ•°æ®...")
        
        # 1. åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†
        print("\nğŸ“ˆ åŠ è½½è®­ç»ƒæ•°æ®...")
        self.train_dataset = SegmentBasedDataset(
            data_folder=self.train_folder,
            window_size=self.window_size,
            time_step=self.time_step,
            sensor_cols_range=self.sensor_cols_range,
            angle_cols_range=self.angle_cols_range
        )
        
        print("\nğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
        self.test_dataset = SegmentBasedDataset(
            data_folder=self.test_folder,
            window_size=self.window_size,
            time_step=self.time_step,
            sensor_cols_range=self.sensor_cols_range,
            angle_cols_range=self.angle_cols_range
        )
        
        # 2. è·å–æ‰€æœ‰æ•°æ®ç”¨äºå½’ä¸€åŒ–
        train_x, train_y = self.train_dataset.get_all_data()
        test_x, test_y = self.test_dataset.get_all_data()
        
        print(f"\nğŸ“ æ•°æ®å½¢çŠ¶ä¿¡æ¯:")
        print(f"   è®­ç»ƒé›†: {train_x.shape} -> {train_y.shape}")
        print(f"   æµ‹è¯•é›†: {test_x.shape} -> {test_y.shape}")
        
        # 3. æ•°æ®å½’ä¸€åŒ–
        print("\nğŸ”§ è¿›è¡Œæ•°æ®å½’ä¸€åŒ–...")
        self._normalize_data(train_x, train_y, test_x, test_y)
        
        # 4. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print("\nğŸ“¦ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
        self._create_data_loaders()
        
        print("\nâœ… æ•°æ®åŠ è½½å’Œé¢„å¤„ç†å®Œæˆ!")
        self._print_dataset_summary()
    
    def _normalize_data(self, train_x, train_y, test_x, test_y):
        """æ•°æ®å½’ä¸€åŒ–å¤„ç†"""
        # é‡å¡‘æ•°æ®ç”¨äºå½’ä¸€åŒ– (åˆå¹¶æ—¶é—´ç»´åº¦)
        train_x_reshaped = train_x.reshape(-1, train_x.shape[-1])  # [num_windows * window_size, sensor_dim]
        test_x_reshaped = test_x.reshape(-1, test_x.shape[-1])
        
        # åˆ›å»ºå½’ä¸€åŒ–å™¨
        self.sensor_scaler = MinMaxScaler()
        self.angle_scaler = MinMaxScaler()
        
        # æ‹Ÿåˆå¹¶è½¬æ¢ä¼ æ„Ÿå™¨æ•°æ®
        train_x_normalized = self.sensor_scaler.fit_transform(train_x_reshaped)
        test_x_normalized = self.sensor_scaler.transform(test_x_reshaped)
        
        # æ‹Ÿåˆå¹¶è½¬æ¢è§’åº¦æ•°æ®
        train_y_normalized = self.angle_scaler.fit_transform(train_y)
        test_y_normalized = self.angle_scaler.transform(test_y)
        
        # é‡å¡‘å›åŸå§‹å½¢çŠ¶
        train_x_normalized = train_x_normalized.reshape(train_x.shape)
        test_x_normalized = test_x_normalized.reshape(test_x.shape)
        
        # æ£€æŸ¥å½’ä¸€åŒ–èŒƒå›´
        self._check_normalization_range()
        
        # æ›´æ–°æ•°æ®é›†
        self.train_dataset.windows_x = train_x_normalized.tolist()
        self.train_dataset.windows_y = train_y_normalized.tolist()
        self.test_dataset.windows_x = test_x_normalized.tolist()
        self.test_dataset.windows_y = test_y_normalized.tolist()
        
        # ä¿å­˜å½’ä¸€åŒ–å™¨
        self._save_scalers()
    
    def _check_normalization_range(self):
        """æ£€æŸ¥å½’ä¸€åŒ–æ•°æ®èŒƒå›´"""
        sensor_range = self.sensor_scaler.data_max_ - self.sensor_scaler.data_min_
        angle_range = self.angle_scaler.data_max_ - self.angle_scaler.data_min_
        
        if np.any(sensor_range == 0):
            zero_cols = np.where(sensor_range == 0)[0]
            print(f"   âš ï¸ è­¦å‘Š: ä¼ æ„Ÿå™¨æ•°æ®ç¬¬ {zero_cols} åˆ—å–å€¼èŒƒå›´ä¸º0")
        
        if np.any(angle_range == 0):
            zero_cols = np.where(angle_range == 0)[0]
            print(f"   âš ï¸ è­¦å‘Š: è§’åº¦æ•°æ®ç¬¬ {zero_cols} åˆ—å–å€¼èŒƒå›´ä¸º0")
    
    def _save_scalers(self):
        """ä¿å­˜å½’ä¸€åŒ–å™¨"""
        sensor_scaler_path = self.scaler_save_dir / f"sensor_scaler_{self.trial_id}.pkl"
        angle_scaler_path = self.scaler_save_dir / f"angle_scaler_{self.trial_id}.pkl"
        
        with open(sensor_scaler_path, 'wb') as f:
            pickle.dump(self.sensor_scaler, f)
        
        with open(angle_scaler_path, 'wb') as f:
            pickle.dump(self.angle_scaler, f)
        
        print(f"   âœ“ ä¼ æ„Ÿå™¨å½’ä¸€åŒ–å™¨å·²ä¿å­˜: {sensor_scaler_path}")
        print(f"   âœ“ è§’åº¦å½’ä¸€åŒ–å™¨å·²ä¿å­˜: {angle_scaler_path}")
    
    def _create_data_loaders(self):
        """åˆ›å»ºPyTorchæ•°æ®åŠ è½½å™¨"""
        # è®­ç»ƒé›†åˆ’åˆ†ï¼ˆè®­ç»ƒ/éªŒè¯ï¼‰
        train_size = len(self.train_dataset)
        val_size = int(train_size * self.validation_split)
        train_size = train_size - val_size
        
        train_subset, val_subset = torch.utils.data.random_split(
            self.train_dataset, [train_size, val_size]
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            train_subset, 
            batch_size=self.batch_size, 
            shuffle=True
        )
        
        self.val_loader = DataLoader(
            val_subset, 
            batch_size=self.batch_size, 
            shuffle=False
        )
        
        self.test_loader = DataLoader(
            self.test_dataset, 
            batch_size=self.batch_size, 
            shuffle=False
        )
    
    def _print_dataset_summary(self):
        """æ‰“å°æ•°æ®é›†æ‘˜è¦ä¿¡æ¯"""
        print(f"\nğŸ“‹ æ•°æ®é›†æ‘˜è¦:")
        print(f"   è®­ç»ƒçª—å£æ•°: {len(self.train_loader.dataset)}")
        print(f"   éªŒè¯çª—å£æ•°: {len(self.val_loader.dataset)}")
        print(f"   æµ‹è¯•çª—å£æ•°: {len(self.test_dataset)}")
        print(f"   æ‰¹å¤„ç†å¤§å°: {self.batch_size}")
        print(f"   ä¼ æ„Ÿå™¨ç»´åº¦: {self.sensor_cols_range[1] - self.sensor_cols_range[0]}")
        print(f"   è§’åº¦ç»´åº¦: {self.angle_cols_range[1] - self.angle_cols_range[0]}")
        print(f"   æ—¶é—´çª—å£å¤§å°: {self.window_size}")
        print(f"   æ—¶é—´æ­¥é•¿: {self.time_step}")
    
    def get_data_loaders(self):
        """è·å–æ•°æ®åŠ è½½å™¨"""
        return self.train_loader, self.val_loader, self.test_loader
    
    def get_scalers(self):
        """è·å–å½’ä¸€åŒ–å™¨"""
        return self.sensor_scaler, self.angle_scaler
    
    def get_datasets(self):
        """è·å–åŸå§‹æ•°æ®é›†"""
        return self.train_dataset, self.test_dataset


# ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # é…ç½®å‚æ•°
    config = {
        'train_folder': './data/model_training/train',
        'test_folder': './data/model_training/test', 
        'window_size': 60,
        'time_step': 3,
        'batch_size': 256,
        'validation_split': 0.2,
        'trial_id': 'segment_based_example'
    }
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    data_loader = SegmentBasedDataLoader(**config)
    
    # åŠ è½½å’Œå‡†å¤‡æ•°æ®
    data_loader.load_and_prepare_data()
    
    # è·å–æ•°æ®åŠ è½½å™¨
    train_loader, val_loader, test_loader = data_loader.get_data_loaders()
    
    # è·å–å½’ä¸€åŒ–å™¨
    sensor_scaler, angle_scaler = data_loader.get_scalers()
    
    print("\nğŸ¯ å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†!")
    
    return train_loader, val_loader, test_loader, sensor_scaler, angle_scaler


if __name__ == "__main__":
    example_usage() 