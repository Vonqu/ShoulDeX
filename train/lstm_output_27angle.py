import os
import glob
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt
import random
from tqdm import tqdm
from predict_utilis import predict_by_batch
import pickle
import math

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)
torch.backends.cudnn.benchmark = True

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# æ¨¡å‹å®šä¹‰
#class LSTM(nn.Module):
#    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
#        super(LSTM, self).__init__()
#        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
#        self.ln = nn.LayerNorm(hidden_size)
#        self.fc = nn.Sequential(
#            nn.Linear(hidden_size, 128),
#            nn.ReLU(),
#            nn.Dropout(dropout),
#            nn.Linear(128, output_size)
#        )

#    def forward(self, x):
#        if x.dim() == 2:
#            x = x.unsqueeze(1)
#        x, _ = self.lstm(x)
#        x = self.ln(x[:, -1, :])
#        x = self.fc(x)
#        return x

# âœ… Multi-Head LSTM
class MultiHeadLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, dropout, output_size):
        super(MultiHeadLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.ln = nn.LayerNorm(hidden_size)
        self.shared_fc = nn.Sequential(
            nn.Linear(hidden_size, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
        )
        # æ¯ä¸ªè§’åº¦ä¸€ä¸ªè¾“å‡ºå¤´
        self.heads = nn.ModuleList([nn.Linear(128, 1) for _ in range(output_size)])

    def forward(self, x):
        if x.dim() == 2:
            x = x.unsqueeze(1)
        x, _ = self.lstm(x)
        x = self.ln(x[:, -1, :])
        x = self.shared_fc(x)
        outputs = [head(x) for head in self.heads]  # æ¯ä¸ªè¾“å‡º shape: [batch_size, 1]
        return torch.cat(outputs, dim=1)            # æ‹¼æˆ [batch_size, 9]

## âœ… Multi-Head LSTM with per-head Input Projection
#class MultiHeadLSTM(nn.Module):
#    def __init__(self, input_size, hidden_size, num_layers, dropout, output_size):
#        super(MultiHeadLSTM, self).__init__()
#        self.output_size = output_size
#        self.input_projections = nn.ModuleList([
#            nn.Linear(input_size, input_size) for _ in range(output_size)
#        ])

#        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
#        self.ln = nn.LayerNorm(hidden_size)

#        self.shared_fc = nn.Sequential(
#            nn.Linear(hidden_size, 128),
#            nn.ReLU(),
#            nn.Dropout(dropout),
#        )

#        self.heads = nn.ModuleList([
#            nn.Linear(128, 1) for _ in range(output_size)
#        ])

#    def forward(self, x):  # x: [B, T, C]
#        outputs = []
#        for i in range(self.output_size):
#            x_proj = self.input_projections[i](x)         # æ¯ä¸ªè§’åº¦ä½¿ç”¨ç‹¬ç«‹æŠ•å½±åçš„è¾“å…¥
#            x_lstm, _ = self.lstm(x_proj)                  # [B, T, H]
#            x_last = self.ln(x_lstm[:, -1, :])             # [B, H]
#            x_feat = self.shared_fc(x_last)                # [B, 128]
#            out = self.heads[i](x_feat)                    # [B, 1]
#            outputs.append(out)
#        return torch.cat(outputs, dim=1)  # [B, output_size]

id_try = 'double_side_27_angles'
# åŠ è½½æ•°æ®
#data_folder = './motion_0407/2/all'
#data_folder = './motion_0407/2/all-clear'
#data_folder = './motion_0407/rdm/alls'
#data_folder = './motion_0407/alls'
#data_folder = './motion_0407/2/zs+mjq'
#data_folder = './motion_0407/2/zss/clear'
#data_folder = './motion_0710/alls'
data_folder = './data\model_training\\train'
# data_folder = './data\dft\qnc\mixed\\train'
csv_files = glob.glob(os.path.join(data_folder, '*.csv'))
random.shuffle(csv_files)
print(f"Found {len(csv_files)} CSV files.")

data_list = []
for file in csv_files:
    df = pd.read_csv(file)
#     #expected_sensor_cols = ['s1', 's2', 's3', 's4', 's5', 's6']
    # expected_sensor_cols = [f's{i}' for i in range(1, 15)]  # s1 åˆ° s14
    # # expected_sensor_cols = df[]

    # # expected_angle_cols = [f'angle{i}' for i in range(1, 19)]  # angle1 åˆ° angle14
    # expected_angle_cols = df[['Frame', 'Time',
    #                  # Angle 1-3: å·¦è‚±éª¨ä¸èƒ¸å»“åæ ‡ç³»è§’åº¦
    #                  'A_humerus_l_thorax_X', 'A_humerus_l_thorax_Y', 'A_humerus_l_thorax_Z',
    #                  # Angle 4-6: å³è‚±éª¨ä¸èƒ¸å»“åæ ‡ç³»è§’åº¦
    #                  'A_humerus_r_thorax_X', 'A_humerus_r_thorax_Y', 'A_humerus_r_thorax_Z',
    #                  # Angle 7-9: å·¦è‚©èƒ›éª¨ä¸èƒ¸å»“åæ ‡ç³»è§’åº¦
    #                  'A_scapula_l_thorax_X', 'A_scapula_l_thorax_Y', 'A_scapula_l_thorax_Z',
    #                  # Angle 10-12: å³è‚©èƒ›éª¨ä¸èƒ¸å»“åæ ‡ç³»è§’åº¦
    #                  'A_scapula_r_thorax_X', 'A_scapula_r_thorax_Y', 'A_scapula_r_thorax_Z',
    #                  # Angle 13-15: å·¦é”éª¨ä¸èƒ¸å»“åæ ‡ç³»è§’åº¦
    #                  'A_clavicle_l_thorax_X', 'A_clavicle_l_thorax_Y', 'A_clavicle_l_thorax_Z',
    #                  # Angle 16-18: å³é”éª¨ä¸èƒ¸å»“åæ ‡ç³»è§’åº¦
    #                  'A_clavicle_r_thorax_X', 'A_clavicle_r_thorax_Y', 'A_clavicle_r_thorax_Z',
    #                  # Angle 19-21: å·¦è‚±éª¨ä¸å·¦è‚©èƒ›éª¨åæ ‡ç³»è§’åº¦
    #                  'A_humerus_l_scapula_X', 'A_humerus_l_scapula_Y', 'A_humerus_l_scapula_Z',
    #                  # Angle 22-24: å³è‚±éª¨ä¸å³è‚©èƒ›éª¨åæ ‡ç³»è§’åº¦
    #                  'A_humerus_r_scapula_X', 'A_humerus_r_scapula_Y', 'A_humerus_r_scapula_Z',
    #                  # Angle 25-27: èƒ¸å»“ä¸é«‹å…³èŠ‚åæ ‡ç³»è§’åº¦
    #                  'A_thorax_hip_X', 'A_thorax_hip_Y', 'A_thorax_hip_Z']]
    # missing_cols = [col for col in expected_sensor_cols + expected_angle_cols if col not in df.columns]
    # if missing_cols:
    
    # æ£€æŸ¥æ•°æ®åˆ—æ•°
    if len(df.columns) < 42:  # 1åˆ—æ—¶é—´ + 14åˆ—ä¼ æ„Ÿå™¨ + 27åˆ—è§’åº¦ = 42åˆ—
        print(f"Warning: {file} has insufficient columns: {len(df.columns)}")
        continue
    
    # è·å–ä¼ æ„Ÿå™¨åˆ—ï¼ˆç¬¬2-15åˆ—ï¼Œç´¢å¼•1-14ï¼‰
    expected_sensor_cols = df.columns[1:15].tolist()  # s1 åˆ° s14
    
    # è·å–è§’åº¦åˆ—ï¼ˆç¬¬16-42åˆ—ï¼Œç´¢å¼•15-41ï¼‰
    expected_angle_cols = df.columns[15:42].tolist()  # 27ä¸ªè§’åº¦åˆ—
    
    # æ£€æŸ¥åˆ—åæ˜¯å¦åŒ…å«é¢„æœŸçš„å…³é”®è¯
    if not all('s' in col for col in expected_sensor_cols):
        print(f"Warning: {file} sensor columns may not be correct: {expected_sensor_cols}")
        continue
    
    if not all(any(keyword in col for keyword in ['A_', 'angle']) for col in expected_angle_cols):
        print(f"Warning: {file} angle columns may not be correct: {expected_angle_cols}")
        continue
    
    data_list.append(df)

data_all = pd.concat(data_list, ignore_index=True)
print(f"Combined data shape: {data_all.shape}")


sensor_data = data_all[expected_sensor_cols].values
angle_data = data_all[expected_angle_cols].values


# æ£€æŸ¥å¹¶æ¸…é™¤ NaN / Inf
sensor_data = np.nan_to_num(sensor_data, nan=0.0, posinf=0.0, neginf=0.0)
angle_data = np.nan_to_num(angle_data, nan=0.0, posinf=0.0, neginf=0.0)

# æ»‘åŠ¨çª—å£å‡½æ•°
def create_dataset(X, y, window_length, time_steps):
    Xs, ys = [], []
    for i in range(int((len(X) - window_length) / time_steps)):
        Xs.append(X[i * time_steps: (i * time_steps + window_length)])
        ys.append(y[i * time_steps + window_length])
    return np.array(Xs), np.array(ys)

# å‚æ•°è®¾ç½®
window_length = 60
time_steps = 3
#window_length = 250
#time_steps = 5

X_all_np, y_all_np = create_dataset(sensor_data, angle_data, window_length, time_steps)
print(f"X_all shape: {X_all_np.shape}, y_all shape: {y_all_np.shape}")

# æ—¶é—´åˆ’åˆ†
split_index = int(len(X_all_np) * 0.8)
X_train_np, y_train_np = X_all_np[:split_index], y_all_np[:split_index]
X_val_np, y_val_np = X_all_np[split_index:], y_all_np[split_index:]

# æ ‡å‡†åŒ–å™¨ä»…åœ¨è®­ç»ƒé›†ä¸Š fit
#scaler_sensor = StandardScaler().fit(X_train_np.reshape(-1, 6))
scaler_sensor = StandardScaler().fit(X_train_np.reshape(-1, 14))
scaler_angle = StandardScaler().fit(y_train_np)

# æ£€æŸ¥æ ‡å‡†å·®ä¸º 0 çš„åˆ—ï¼ˆä¼šå¯¼è‡´é™¤ä»¥ 0ï¼‰
if np.any(scaler_sensor.scale_ == 0):
    raise ValueError("âš ï¸ ä¼ æ„Ÿå™¨æ•°æ®ä¸­å­˜åœ¨æ ‡å‡†å·®ä¸º 0 çš„åˆ—ï¼Œæ— æ³•æ ‡å‡†åŒ–ï¼")

# ä¿å­˜ scaler
with open(f'./predict/sensor_scaler{id_try}.pkl', 'wb') as f:
    pickle.dump(scaler_sensor, f)
with open(f'./predict/angle_scaler{id_try}.pkl', 'wb') as f:
    pickle.dump(scaler_angle, f)

# æ ‡å‡†åŒ–æ•°æ®
#X_train_np = scaler_sensor.transform(X_train_np.reshape(-1, 6)).reshape(-1, window_length, 6)
#X_val_np = scaler_sensor.transform(X_val_np.reshape(-1, 6)).reshape(-1, window_length, 6)
X_train_np = scaler_sensor.transform(X_train_np.reshape(-1, 14)).reshape(-1, window_length, 14)
X_val_np = scaler_sensor.transform(X_val_np.reshape(-1, 14)).reshape(-1, window_length, 14)
y_train_np = scaler_angle.transform(y_train_np)
y_val_np = scaler_angle.transform(y_val_np)

# è½¬ä¸ºå¼ é‡
X_train = torch.from_numpy(X_train_np).float().to(device)
y_train = torch.from_numpy(y_train_np).float().to(device)
X_val = torch.from_numpy(X_val_np).float().to(device)
y_val = torch.from_numpy(y_val_np).float().to(device)

print(f"Train sequences: {X_train.shape[0]}, Validation sequences: {X_val.shape[0]}")

train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=256, shuffle=True)
val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=256, shuffle=False)

# æ¨¡å‹ä¸ä¼˜åŒ–å™¨
input_size = X_train.shape[-1] #åº”è¯¥æ˜¯14
output_size = y_train.shape[1] #åº”è¯¥æ˜¯18
print(f"Input size: {input_size}, Output size: {output_size}")
#model = LSTM(input_size, hidden_size=256, num_layers=3, output_size=output_size, dropout=0.1).to(device)
model = MultiHeadLSTM(input_size, hidden_size=256, num_layers=3, dropout=0.1, output_size=output_size).to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)

# è®­ç»ƒ
train_losses, val_losses = [], []
for epoch in range(70):
    model.train()
    train_loss_sum = 0
    for X_batch, y_batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/10"):
        optimizer.zero_grad()
        preds = model(X_batch)
        loss = criterion(preds, y_batch) + 0.0003 * sum(torch.norm(p, 2) for p in model.parameters())
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # ğŸ”§ æ¢¯åº¦è£å‰ª
        optimizer.step()
        train_loss_sum += loss.item()
    train_losses.append(train_loss_sum / len(train_loader))

    #model.eval()
    #val_loss_sum = 0
    #with torch.no_grad():
    #    for X_batch, y_batch in val_loader:
    #        preds = model(X_batch)
    #        loss = criterion(preds, y_batch)
    #        val_loss_sum += loss.item()
    #val_losses.append(val_loss_sum / len(val_loader))
    #print(f"[Epoch {epoch+1}] Train Loss: {train_losses[-1]:.6f} | Val Loss: {val_losses[-1]:.6f}")
    #scheduler.step()

        # éªŒè¯é˜¶æ®µ
    model.eval()
    val_loss_sum = 0
    per_angle_losses = []

    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            preds = model(X_batch)

            # åŸå§‹æ•´ä½“ loss
            loss = criterion(preds, y_batch)
            val_loss_sum += loss.item()

            # ğŸ‘‰ æ¯ä¸ªè§’åº¦å•ç‹¬ loss
            angle_losses = [nn.functional.mse_loss(preds[:, i], y_batch[:, i]).item() for i in range(output_size)]
            per_angle_losses.append(angle_losses)

    val_losses.append(val_loss_sum / len(val_loader))

    # ğŸ‘‰ æ‰“å°æ¯ä¸ªè§’åº¦çš„å¹³å‡éªŒè¯ loss
    mean_angle_losses = np.mean(per_angle_losses, axis=0)
    print(f"[Epoch {epoch+1}] Train Loss: {train_losses[-1]:.6f} | Val Loss: {val_losses[-1]:.6f}")
    for i, l in enumerate(mean_angle_losses):
        print(f"ğŸ“ Angle {i+1} Val Loss: {l:.4f}")

    scheduler.step()

    

# RMSE å‡½æ•°
def compute_rmse(y_true, y_pred):
    return [np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i])) for i in range(y_true.shape[1])]

# è®­ç»ƒé›†è¯„ä¼°
model.eval()
train_preds = predict_by_batch(model, X_train, batch_size=256)
train_true = y_train.detach().cpu().numpy()
train_preds_denorm = scaler_angle.inverse_transform(train_preds)
train_true_denorm = scaler_angle.inverse_transform(train_true)
rmse_train = compute_rmse(train_true_denorm, train_preds_denorm)
print("\nğŸ” Train RMSE (degrees):", rmse_train)
print(f"ğŸ¯ Average Train RMSE: {np.mean(rmse_train):.4f}")

# éªŒè¯é›†è¯„ä¼°
val_preds = predict_by_batch(model, X_val, batch_size=256)
val_true = y_val.detach().cpu().numpy()
val_preds_denorm = scaler_angle.inverse_transform(val_preds)
val_true_denorm = scaler_angle.inverse_transform(val_true)
rmse_val = compute_rmse(val_true_denorm, val_preds_denorm)
print("\nğŸ§ª Validation RMSE (degrees):", rmse_val)
print(f"ğŸ“Š Average Validation RMSE: {np.mean(rmse_val):.4f}")

# ä¿å­˜æ¨¡å‹
# torch.save(model.state_dict(), "lstm_model_db.ckpt")
model_path = f"./model/lstm_model_db{id_try}.pth"
torch.save(model.state_dict(), model_path)
print("\nâœ… æ¨¡å‹å·²ä¿å­˜")

# æŸå¤±æ›²çº¿
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss', linestyle='--')
plt.plot(val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()
plt.show()

# è®­ç»ƒé›†é¢„æµ‹å›¾
#fig, axes = plt.subplots(6, 3, figsize=(18, 12))  # 6è¡Œ3åˆ— = 18ä¸ªå›¾
num_plots = 27
# è‡ªåŠ¨ç¡®å®šåˆ—æ•°ï¼ˆä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨æŒ‡å®šåˆ—æ•°ï¼‰
cols = 3  # ä¾‹å¦‚ä¸€è¡Œæ”¾4ä¸ªå›¾ï¼Œå¤Ÿç´§å‡‘
rows = math.ceil(num_plots / cols)

fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 3))  # åŠ¨æ€å°ºå¯¸
axes = axes.flatten()  # è½¬æˆä¸€ç»´æ–¹ä¾¿è®¿é—®

for i in range(train_true.shape[1]):
    ax = axes.flat[i]
    ax.plot(train_true_denorm[:, i], label='Actual', color='blue')
    ax.plot(train_preds_denorm[:, i], label='Predicted', color='orange')
    ax.set_xlabel('Frame')
    ax.set_ylabel('Angle')
    ax.set_title(f'Train - Angle {i+1}')
plt.tight_layout()
plt.show()

# éªŒè¯é›†é¢„æµ‹å›¾
#fig, axes = plt.subplots(6, 3, figsize=(18, 12))  # 6 rows Ã— 3 columns = 18 plots
num_plots = 27 # è‡ªåŠ¨ç¡®å®šåˆ—æ•°ï¼ˆä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨æŒ‡å®šåˆ—æ•°ï¼‰
cols = 3  # ä¾‹å¦‚ä¸€è¡Œæ”¾4ä¸ªå›¾ï¼Œå¤Ÿç´§å‡‘
rows = math.ceil(num_plots / cols)
fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 3))  # åŠ¨æ€å°ºå¯¸
axes = axes.flatten()  # è½¬æˆä¸€ç»´æ–¹ä¾¿è®¿é—®

for i in range(val_true.shape[1]):
    ax = axes.flat[i]
    ax.plot(val_true_denorm[:, i], label='Actual', color='blue')
    ax.plot(val_preds_denorm[:, i], label='Predicted', color='orange')
    ax.set_xlabel('Frame')
    ax.set_ylabel('Angle')
    ax.set_title(f'Test - Angle {i+1}')
    ax.legend()
plt.tight_layout()
plt.show()

# =============================
# â–¶ï¸ æµ‹è¯•é›†æ¨ç† & è¯„ä¼°
# =============================

#test_folder = './motion_0407/2/mjq/clear'
#test_folder = './motion_0407/2/all-clear'
#test_folder = './motion_0407/rdm/slla'
#test_folder = './motion_0710/slla'
test_folder = './data/model_training/test'
#test_folder = './motion_0407/rdm/slla/clear'
#test_folder = './motion_0407/2/qnc'
#test_folder = './motion_0407/2/zss/clear'
test_csv_files = glob.glob(os.path.join(test_folder, '*.csv'))
print(f"\nğŸ§ª Found {len(test_csv_files)} test CSV files.")

# åŠ è½½å¹¶åˆå¹¶æµ‹è¯•æ•°æ®
test_list = []
for file in test_csv_files:
    df = pd.read_csv(file)
    
    # æ£€æŸ¥æ•°æ®åˆ—æ•°
    if len(df.columns) < 42:  # 1åˆ—æ—¶é—´ + 14åˆ—ä¼ æ„Ÿå™¨ + 27åˆ—è§’åº¦ = 42åˆ—
        print(f"Warning: {file} has insufficient columns: {len(df.columns)}")
        continue
    
    # è·å–ä¼ æ„Ÿå™¨åˆ—ï¼ˆç¬¬2-15åˆ—ï¼Œç´¢å¼•1-14ï¼‰
    expected_sensor_cols = df.columns[1:15].tolist()  # s1 åˆ° s14
    
    # è·å–è§’åº¦åˆ—ï¼ˆç¬¬16-42åˆ—ï¼Œç´¢å¼•15-41ï¼‰
    expected_angle_cols = df.columns[15:42].tolist()  # 27ä¸ªè§’åº¦åˆ—
    
    # æ£€æŸ¥åˆ—åæ˜¯å¦åŒ…å«é¢„æœŸçš„å…³é”®è¯
    if not all('s' in col for col in expected_sensor_cols):
        print(f"Warning: {file} sensor columns may not be correct: {expected_sensor_cols}")
        continue
    
    if not all(any(keyword in col for keyword in ['A_', 'angle']) for col in expected_angle_cols):
        print(f"Warning: {file} angle columns may not be correct: {expected_angle_cols}")
        continue
    
    test_list.append(df)

if len(test_list) == 0:
    raise ValueError("âŒ No valid test files found.")

test_data = pd.concat(test_list, ignore_index=True)
sensor_test = test_data[expected_sensor_cols].values
angle_test = test_data[expected_angle_cols].values

# æ¸…æ´— NaN / Inf
sensor_test = np.nan_to_num(sensor_test, nan=0.0, posinf=0.0, neginf=0.0)
angle_test = np.nan_to_num(angle_test, nan=0.0, posinf=0.0, neginf=0.0)

# åˆ›å»ºæ»‘åŠ¨çª—å£
X_test_np, y_test_np = create_dataset(sensor_test, angle_test, window_length, time_steps)

# åŠ è½½è®­ç»ƒæ—¶ä¿å­˜çš„ scaler
with open(f'./predict/sensor_scaler{id_try}.pkl', 'rb') as f:
    scaler_sensor = pickle.load(f)
with open(f'./predict/angle_scaler{id_try}.pkl', 'rb') as f:
    scaler_angle = pickle.load(f)

# æ ‡å‡†åŒ–æµ‹è¯•é›†ï¼ˆæ³¨æ„ï¼šä½¿ç”¨è®­ç»ƒé›†çš„ scalerï¼‰
X_test_np = scaler_sensor.transform(X_test_np.reshape(-1, 14)).reshape(-1, window_length, 14)
y_test_np = scaler_angle.transform(y_test_np)

# è½¬ä¸ºå¼ é‡
X_test = torch.from_numpy(X_test_np).float().to(device)
y_test = torch.from_numpy(y_test_np).float().to(device)

## åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
#model = LSTM(input_size, hidden_size=256, num_layers=3, output_size=output_size, dropout=0.1).to(device)
#model.load_state_dict(torch.load("lstm_model_SingleAction_timeSplit.pth"))
#model.eval()

# åŠ è½½è®­ç»ƒå¥½çš„ MultiHead æ¨¡å‹
model = MultiHeadLSTM(input_size, hidden_size=256, num_layers=3, dropout=0.1, output_size=output_size).to(device)
model.load_state_dict(torch.load(f"./model/lstm_model_db{id_try}.pth"))
model.eval()


# æµ‹è¯•é›†é¢„æµ‹
test_preds = predict_by_batch(model, X_test, batch_size=256)
test_true = y_test.detach().cpu().numpy()

# åæ ‡å‡†åŒ–
test_preds_denorm = scaler_angle.inverse_transform(test_preds)
test_true_denorm = scaler_angle.inverse_transform(test_true)

## è®¡ç®— RMSE
#rmse_test = compute_rmse(test_true_denorm, test_preds_denorm)
#avg_rmse_test = np.mean(rmse_test)

#print("\nğŸ§ª Test RMSE (degrees):", rmse_test)
#print(f"ğŸ¯ Average Test RMSE: {avg_rmse_test:.4f}")

# è®¡ç®—æ¯ä¸ªè§’åº¦çš„ RMSE
rmse_test = compute_rmse(test_true_denorm, test_preds_denorm)
avg_rmse_test = np.mean(rmse_test)

print("\nğŸ§ª Test RMSE (degrees):", rmse_test)
for i, rmse in enumerate(rmse_test):
    print(f"ğŸ“ Angle {i+1} Test RMSE: {rmse:.4f}")
print(f"ğŸ¯ Average Test RMSE: {avg_rmse_test:.4f}")


# å¯è§†åŒ–æµ‹è¯•é›†é¢„æµ‹ç»“æœ
#fig, axes = plt.subplots(6, 3, figsize=(18, 12))
num_plots = 27
# è‡ªåŠ¨ç¡®å®šåˆ—æ•°ï¼ˆä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨æŒ‡å®šåˆ—æ•°ï¼‰
cols = 3  # ä¾‹å¦‚ä¸€è¡Œæ”¾4ä¸ªå›¾ï¼Œå¤Ÿç´§å‡‘
rows = math.ceil(num_plots / cols)
fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 3))  # åŠ¨æ€å°ºå¯¸
axes = axes.flatten()  # è½¬æˆä¸€ç»´æ–¹ä¾¿è®¿é—®
for i in range(test_true.shape[1]):
    ax = axes.flat[i]
    ax.plot(test_true_denorm[:, i], label='Actual', color='blue')
    ax.plot(test_preds_denorm[:, i], label='Predicted', color='orange')
    ax.set_xlabel('Frame')
    ax.set_ylabel('Angle')
    ax.set_title(f'Test - Angle {i+1}')
    ax.legend()
plt.tight_layout()
plt.show()